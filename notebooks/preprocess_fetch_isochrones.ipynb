{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8fcd65",
   "metadata": {},
   "source": [
    "First subsample (stratified) in case we don't have enough time to make enough requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf45332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xj/ny4zmnqd609bv2rpjzzr11rm0000gn/T/ipykernel_19525/2315601584.py:9: DtypeWarning: Columns (1,15,16,17,19,21,22,24,25,31,33,34,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/curated/rent_features/cleaned_listings.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (25687, 40)\n",
      "Original dataset size: 25,687 records\n",
      "\n",
      "Data distribution before sampling:\n",
      "Year distribution:\n",
      "year\n",
      "2022     2620\n",
      "2023      340\n",
      "2024     4065\n",
      "2025    18662\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quarter distribution:\n",
      "quarter\n",
      "3      3357\n",
      "6      5937\n",
      "9     15208\n",
      "12     1185\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique suburbs: 988\n",
      "Unique year-quarter combinations: 15\n",
      "\n",
      "Creating stratification groups...\n",
      "Number of strata: 2721\n",
      "Strata size distribution:\n",
      "count    2721.000000\n",
      "mean        9.440279\n",
      "std        24.438703\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         3.000000\n",
      "75%        11.000000\n",
      "max       846.000000\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Performing stratified sampling...\n",
      "\n",
      "Sampled dataset shape: (12882, 40)\n",
      "Sampled dataset size: 12,882 records\n",
      "Reduction: 49.9%\n",
      "\n",
      "Data distribution after sampling:\n",
      "Year distribution:\n",
      "year\n",
      "2022    1295\n",
      "2023     175\n",
      "2024    2090\n",
      "2025    9322\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quarter distribution:\n",
      "quarter\n",
      "3     1694\n",
      "6     2977\n",
      "9     7596\n",
      "12     615\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique suburbs: 988\n",
      "\n",
      "Verifying stratification:\n",
      "Strata with data after sampling: 2721\n",
      "Sample of strata sizes:\n",
      "suburb      year  quarter\n",
      "abbotsford  2022  6          10\n",
      "            2024  3           9\n",
      "                  6           2\n",
      "                  9           1\n",
      "                  12          1\n",
      "            2025  3           9\n",
      "                  6           1\n",
      "                  9          16\n",
      "aberfeldie  2025  3           1\n",
      "                  9           2\n",
      "dtype: int64\n",
      "\n",
      "Sampled data saved to: ../data/curated/rent_features/cleaned_listings_sampled.csv\n",
      "\n",
      "Summary:\n",
      "Original records: 25,687\n",
      "Sampled records: 12,882\n",
      "Sampling ratio: 50.15%\n",
      "Records removed: 12,805\n"
     ]
    }
   ],
   "source": [
    "# Stratified sampling script for rental listings data\n",
    "# Performs stratified sampling by suburb, year, and quarter to reduce data to 50%\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/curated/rent_features/cleaned_listings.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Original dataset size: {len(df):,} records\")\n",
    "\n",
    "# Display data distribution before sampling\n",
    "print(\"\\nData distribution before sampling:\")\n",
    "print(\"Year distribution:\")\n",
    "print(df['year'].value_counts().sort_index())\n",
    "print(\"\\nQuarter distribution:\")\n",
    "print(df['quarter'].value_counts().sort_index())\n",
    "print(f\"\\nUnique suburbs: {df['suburb'].nunique()}\")\n",
    "print(f\"Unique year-quarter combinations: {df[['year', 'quarter']].drop_duplicates().shape[0]}\")\n",
    "\n",
    "# Create stratification groups\n",
    "print(\"\\nCreating stratification groups...\")\n",
    "df['strata'] = df['suburb'].astype(str) + '_' + df['year'].astype(str) + '_' + df['quarter'].astype(str)\n",
    "strata_counts = df['strata'].value_counts()\n",
    "print(f\"Number of strata: {len(strata_counts)}\")\n",
    "print(f\"Strata size distribution:\")\n",
    "print(strata_counts.describe())\n",
    "\n",
    "# Perform stratified sampling\n",
    "print(\"\\nPerforming stratified sampling...\")\n",
    "sampled_dfs = []\n",
    "\n",
    "for stratum in df['strata'].unique():\n",
    "    stratum_data = df[df['strata'] == stratum]\n",
    "    \n",
    "    # Calculate sample size (50% of stratum)\n",
    "    sample_size = max(1, len(stratum_data) // 2)  # Ensure at least 1 record per stratum\n",
    "    \n",
    "    # If stratum has only 1 record, keep it\n",
    "    if len(stratum_data) == 1:\n",
    "        sampled_dfs.append(stratum_data)\n",
    "    else:\n",
    "        # Random sampling within stratum\n",
    "        sampled_stratum = stratum_data.sample(n=sample_size, random_state=42)\n",
    "        sampled_dfs.append(sampled_stratum)\n",
    "\n",
    "# Combine all sampled strata\n",
    "sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "# Remove the temporary strata column\n",
    "sampled_df = sampled_df.drop('strata', axis=1)\n",
    "\n",
    "print(f\"\\nSampled dataset shape: {sampled_df.shape}\")\n",
    "print(f\"Sampled dataset size: {len(sampled_df):,} records\")\n",
    "print(f\"Reduction: {((len(df) - len(sampled_df)) / len(df) * 100):.1f}%\")\n",
    "\n",
    "# Display data distribution after sampling\n",
    "print(\"\\nData distribution after sampling:\")\n",
    "print(\"Year distribution:\")\n",
    "print(sampled_df['year'].value_counts().sort_index())\n",
    "print(\"\\nQuarter distribution:\")\n",
    "print(sampled_df['quarter'].value_counts().sort_index())\n",
    "print(f\"\\nUnique suburbs: {sampled_df['suburb'].nunique()}\")\n",
    "\n",
    "# Verify stratification is maintained\n",
    "print(\"\\nVerifying stratification:\")\n",
    "strata_verification = sampled_df.groupby(['suburb', 'year', 'quarter']).size()\n",
    "print(f\"Strata with data after sampling: {len(strata_verification)}\")\n",
    "print(\"Sample of strata sizes:\")\n",
    "print(strata_verification.head(10))\n",
    "\n",
    "# Save the sampled data\n",
    "output_path = '../data/curated/rent_features/cleaned_listings_sampled.csv'\n",
    "sampled_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSampled data saved to: {output_path}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Original records: {len(df):,}\")\n",
    "print(f\"Sampled records: {len(sampled_df):,}\")\n",
    "print(f\"Sampling ratio: {len(sampled_df)/len(df):.2%}\")\n",
    "print(f\"Records removed: {len(df) - len(sampled_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502acd7",
   "metadata": {},
   "source": [
    "Because the ORS isochrone is limited to 500 requests. We will rerun this notebook and update the `curated/rent_features/cleaned_listings.csv` as we impute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8be98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read in the subsampled data\n",
    "df = pd.read_csv('../data/curated/rent_features/cleaned_listings_sampled.csv')\n",
    "\n",
    "# Create a new column for isochrones and fill with NaN, we expect 6 columns (3 for driving, 3 for walking with fixed time of 5, 10, 15 minutes respectively)\n",
    "df[['driving_5min', 'driving_10min', 'driving_15min', 'walking_5min', 'walking_10min', 'walking_15min']] = np.nan\n",
    "\n",
    "df[['coordinates', 'driving_5min', 'driving_10min', 'driving_15min', 'walking_5min', 'walking_10min', 'walking_15min']]\n",
    "\n",
    "df.to_csv('../data/raw/missing_isochrones.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46ff4d",
   "metadata": {},
   "source": [
    "# Running API request below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5941f9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "OpenRouteService client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os \n",
    "\n",
    "\n",
    "# Import the GeoUtils class from utils/geo.py\n",
    "from utils.geo import GeoUtils\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "APIKEY1 = os.getenv('ORS_API_KEY1')\n",
    "APIKEY2 = os.getenv('ORS_API_KEY2')\n",
    "APIKEY3 = os.getenv('ORS_API_KEY3')\n",
    "APIKEY4 = os.getenv('ORS_API_KEY4')\n",
    "APIKEY5 = os.getenv('ORS_API_KEY5')\n",
    "# Initialize GeoUtils \n",
    "geoutils = GeoUtils(ors_api_key=APIKEY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a987d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the missing isochrones using geopandas\n",
    "gdf = gpd.read_file('../data/raw/missing_isochrones.csv') \n",
    "\n",
    "# get the first 500 \n",
    "gdf_tmp = gdf.head(500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f246c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample coordinate format: POINT (-37.8058235 144.9940691)\n",
      "Successfully converted WKT POINT strings to Point objects\n",
      "\n",
      "Created geometry column with 500 valid points\n",
      "Sample geometry objects:\n",
      "0    POINT (-37.8058235 144.9940691)\n",
      "1     POINT (-37.8111571 145.008908)\n",
      "2    POINT (-37.8041915 144.9956615)\n",
      "3     POINT (-37.8111571 145.008908)\n",
      "4    POINT (-37.8109899 145.0067059)\n",
      "Name: geometry, dtype: object\n",
      "\n",
      "Geometry type: <class 'shapely.geometry.point.Point'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xj/ny4zmnqd609bv2rpjzzr11rm0000gn/T/ipykernel_19525/2102773548.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gdf_tmp['geometry'] = gdf_tmp['coordinates'].apply(\n"
     ]
    }
   ],
   "source": [
    "# Convert coordinates to Point objects from shapely\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "\n",
    "# Check the format of the coordinates column\n",
    "sample_coords = gdf_tmp['coordinates'].iloc[0]\n",
    "print(f\"Sample coordinate format: {sample_coords}\")\n",
    "\n",
    "# Convert WKT POINT strings to Point objects\n",
    "if isinstance(sample_coords, str) and sample_coords.startswith('POINT'):\n",
    "    # Handle WKT POINT format: \"POINT (lon lat)\"\n",
    "    gdf_tmp['geometry'] = gdf_tmp['coordinates'].apply(\n",
    "        lambda coord: wkt.loads(coord)\n",
    "    )\n",
    "    print(\"Successfully converted WKT POINT strings to Point objects\")\n",
    "else:\n",
    "    print(\"Unexpected coordinate format. Expected WKT POINT format.\")\n",
    "\n",
    "# Verify the conversion worked\n",
    "print(f\"\\nCreated geometry column with {gdf_tmp['geometry'].notna().sum()} valid points\")\n",
    "print(\"Sample geometry objects:\")\n",
    "print(gdf_tmp['geometry'].head())\n",
    "print(f\"\\nGeometry type: {type(gdf_tmp['geometry'].iloc[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07197dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 47 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   property_id          500 non-null    object\n",
      " 1   bedrooms             500 non-null    object\n",
      " 2   bathrooms            500 non-null    object\n",
      " 3   car_spaces           500 non-null    object\n",
      " 4   property_type        500 non-null    object\n",
      " 5   land_area            500 non-null    object\n",
      " 6   property_features    500 non-null    object\n",
      " 7   suburb               500 non-null    object\n",
      " 8   postcode             500 non-null    object\n",
      " 9   year                 500 non-null    object\n",
      " 10  quarter              500 non-null    object\n",
      " 11  age_0_to_19          500 non-null    object\n",
      " 12  age_20_to_39         500 non-null    object\n",
      " 13  age_40_to_59         500 non-null    object\n",
      " 14  age_60_plus          500 non-null    object\n",
      " 15  agency_name          500 non-null    object\n",
      " 16  agent_name           500 non-null    object\n",
      " 17  appointment_only     500 non-null    object\n",
      " 18  avg_days_on_market   500 non-null    object\n",
      " 19  description          500 non-null    object\n",
      " 20  family_percentage    500 non-null    object\n",
      " 21  features_list        500 non-null    object\n",
      " 22  first_listed_date    500 non-null    object\n",
      " 23  full_address         500 non-null    object\n",
      " 24  last_sold_date       500 non-null    object\n",
      " 25  listing_status       500 non-null    object\n",
      " 26  long_term_resident   500 non-null    object\n",
      " 27  median_rent_price    500 non-null    object\n",
      " 28  median_sold_price    500 non-null    object\n",
      " 29  number_sold          500 non-null    object\n",
      " 30  renter_percentage    500 non-null    object\n",
      " 31  schools              500 non-null    object\n",
      " 32  single_percentage    500 non-null    object\n",
      " 33  state_abbreviation   500 non-null    object\n",
      " 34  structured_features  500 non-null    object\n",
      " 35  unit_number          500 non-null    object\n",
      " 36  updated_date         500 non-null    object\n",
      " 37  url                  500 non-null    object\n",
      " 38  coordinates          500 non-null    object\n",
      " 39  weekly_rent          500 non-null    object\n",
      " 40  driving_5min         500 non-null    object\n",
      " 41  driving_10min        500 non-null    object\n",
      " 42  driving_15min        500 non-null    object\n",
      " 43  walking_5min         500 non-null    object\n",
      " 44  walking_10min        500 non-null    object\n",
      " 45  walking_15min        500 non-null    object\n",
      " 46  geometry             500 non-null    object\n",
      "dtypes: object(47)\n",
      "memory usage: 183.7+ KB\n"
     ]
    }
   ],
   "source": [
    "gdf_tmp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef7d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-38.8058235 143.9940691 2.0 2.0\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,289.9881382)\"><circle cx=\"-37.8058235\" cy=\"144.9940691\" r=\"0.06\" stroke=\"#555555\" stroke-width=\"0.02\" fill=\"#66cc99\" opacity=\"0.6\" /></g></svg>"
      ],
      "text/plain": [
       "<POINT (-37.806 144.994)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_tmp[['driving_5min', 'driving_10min', 'driving_15min']] = gdf_tmp['geometry'].apply(lambda x: geo_utils.get_isochrone_with_delay(profile=\"driving-car\", range_values=[300, 600, 900], coordinate=x))\n",
    "# generate a list of columns called isochrone_driving_300, isochrone_driving_600, isochrone_driving_900\n",
    "gdf[['isochrone_driving_5', 'isochrone_driving_10', 'isochrone_driving_15']] = gdf['geometry'].apply(lambda x: geo_utils.get_isochrone_with_delay(profile=\"driving-car\", range_values=[300, 600, 900], coordinate=x))\n",
    "\n",
    "gdf[['isochrone_driving_300', 'isochrone_driving_600', 'isochrone_driving_900']]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

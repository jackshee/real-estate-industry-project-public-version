{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db79881a",
   "metadata": {},
   "source": [
    "## Preprocessing for School Location Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2323c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Directory containing the school CSV files\n",
    "schools_dir = '../data/landing/schools'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(schools_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Load each CSV and collect their columns\n",
    "schemas = {}\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(schools_dir, file), nrows=0, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(os.path.join(schools_dir, file), nrows=0, encoding='latin1')\n",
    "    schemas[file] = set(df.columns)\n",
    "\n",
    "# Display the columns for each file\n",
    "for file, cols in schemas.items():\n",
    "    print(f\"{file}: {sorted(cols)}\")\n",
    "\n",
    "# Check if all schemas match\n",
    "all_schemas = list(schemas.values())\n",
    "schemas_match = all(s == all_schemas[0] for s in all_schemas)\n",
    "print(f\"\\nSchemas match: {schemas_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e980171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pandas show all rows when printing dataframe\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standardized schema based on common columns across all years\n",
    "standard_columns = [\n",
    "    'Address_Line_1', 'Address_Line_2', 'Address_Postcode', 'Address_State', 'Address_Town',\n",
    "    'Education_Sector', 'Entity_Type', 'Full_Phone_No', 'LGA_ID', 'LGA_Name',\n",
    "    'Postal_Address_Line_1', 'Postal_Address_Line_2', 'Postal_Postcode', 'Postal_State', 'Postal_Town',\n",
    "    'School_Name', 'School_No', 'School_Type', 'X', 'Y',\n",
    "    # Additional columns that exist in some years\n",
    "    'Area', 'LGA_TYPE', 'Region', 'School_Status'\n",
    "]\n",
    "\n",
    "print(\"Standardized schema:\")\n",
    "for col in standard_columns:\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc144f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize column names and add missing columns\n",
    "def standardize_school_dataframe(df, year):\n",
    "    \"\"\"\n",
    "    Standardize a school dataframe to have consistent columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame\n",
    "    year: string indicating the year (2023, 2024, or 2025)\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame with standardized columns\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_std = df.copy()\n",
    "    \n",
    "    # Handle column name variations\n",
    "    column_mapping = {\n",
    "        'AREA_Name': 'Area',  # 2024 has AREA_Name instead of Area\n",
    "        'Region_Name': 'Region'  # 2024 has Region_Name instead of Region\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    df_std = df_std.rename(columns=column_mapping)\n",
    "    \n",
    "    # Add missing columns with NaN values\n",
    "    for col in standard_columns:\n",
    "        if col not in df_std.columns:\n",
    "            df_std[col] = None\n",
    "    \n",
    "    # Reorder columns to match standard schema\n",
    "    df_std = df_std[standard_columns]\n",
    "    \n",
    "    # Add year column to indicate when school was established\n",
    "    df_std['establishment_year'] = year\n",
    "    \n",
    "    return df_std\n",
    "\n",
    "# Test the function with a small sample\n",
    "print(\"Testing standardization function...\")\n",
    "for file in csv_files:\n",
    "    year = file.split('_')[-1].split('.')[0]  # Extract year from filename\n",
    "    print(f\"\\nProcessing {file} (year: {year})\")\n",
    "    \n",
    "    # Load a small sample to test\n",
    "    try:\n",
    "        df_sample = pd.read_csv(os.path.join(schools_dir, file), nrows=5, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df_sample = pd.read_csv(os.path.join(schools_dir, file), nrows=5, encoding='latin1')\n",
    "    \n",
    "    # Standardize\n",
    "    df_std = standardize_school_dataframe(df_sample, year)\n",
    "    \n",
    "    print(f\"Original columns: {len(df_sample.columns)}\")\n",
    "    print(f\"Standardized columns: {len(df_std.columns)}\")\n",
    "    print(f\"Missing columns added: {[col for col in standard_columns if col not in df_sample.columns]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c197ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and standardize all school datasets\n",
    "print(\"Loading and standardizing all school datasets...\")\n",
    "standardized_dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    year = file.split('_')[-1].split('.')[0]  # Extract year from filename\n",
    "    print(f\"\\nProcessing {file} (year: {year})\")\n",
    "    \n",
    "    # Load the full dataset\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(schools_dir, file), encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(os.path.join(schools_dir, file), encoding='latin1')\n",
    "    \n",
    "    print(f\"  Loaded {len(df)} schools\")\n",
    "    \n",
    "    # Standardize the dataframe\n",
    "    df_std = standardize_school_dataframe(df, year)\n",
    "    standardized_dfs.append(df_std)\n",
    "    \n",
    "    print(f\"  Standardized to {len(df_std.columns)} columns\")\n",
    "\n",
    "# Combine all standardized dataframes\n",
    "print(f\"\\nCombining {len(standardized_dfs)} datasets...\")\n",
    "combined_schools = pd.concat(standardized_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {combined_schools.shape}\")\n",
    "print(f\"Total schools: {len(combined_schools)}\")\n",
    "print(f\"\\nEstablishment year distribution:\")\n",
    "print(combined_schools['establishment_year'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select School_Name, School_No, School_Type, School_Status, establishment_year, X, Y\n",
    "combined_schools = combined_schools[['School_Name', 'Education_Sector', 'School_Type', 'School_Status', 'establishment_year', 'X', 'Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the X, Y columns to a Point object from shapely \n",
    "combined_schools['coordinates'] = combined_schools.apply(lambda row: Point(row['X'], row['Y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb22ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the X, Y columns to 4 decimal places\n",
    "combined_schools['X'] = combined_schools['X'].round(1)\n",
    "combined_schools['Y'] = combined_schools['Y'].round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_schools['establishment_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_schools['School_Type'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646547bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove schools with the same fuzzy matched name and X and Y duplicates at 1 dp\n",
    "from difflib import get_close_matches\n",
    "to_drop = set()\n",
    "name_to_indices = {}    \n",
    "for idx, row in combined_schools.iterrows():\n",
    "    name = row['School_Name']\n",
    "    x = row['X']\n",
    "    y = row['Y']\n",
    "\n",
    "    # Find close matches to the current school name\n",
    "    close_matches = get_close_matches(name, name_to_indices.keys(), n=1, cutoff=0.99)\n",
    "    \n",
    "    if close_matches:\n",
    "        matched_name = close_matches[0]\n",
    "        for matched_idx in name_to_indices[matched_name]:\n",
    "            matched_row = combined_schools.loc[matched_idx]\n",
    "            if matched_row['X'] == x and matched_row['Y'] == y:\n",
    "                # If X and Y also match, mark the current index for dropping\n",
    "                to_drop.add(idx)\n",
    "                break\n",
    "    \n",
    "    # Add the current index to the mapping\n",
    "    if name not in name_to_indices:\n",
    "        name_to_indices[name] = []\n",
    "    name_to_indices[name].append(idx)  \n",
    "print(f\"Dropping {len(to_drop)} duplicate schools based on fuzzy name and coordinates match.\")\n",
    "combined_schools = combined_schools.drop(index=to_drop).reset_index(drop=True)\n",
    "print(f\"Dataset shape after removing duplicates: {combined_schools.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870944c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by school_name\n",
    "combined_schools = combined_schools.sort_values(by='School_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_schools['School_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'School_Type' where it is 'Language', 'Camp' \n",
    "combined_schools = combined_schools[~combined_schools['School_Type'].isin(['Language', 'Camp'])]\n",
    "\n",
    "combined_schools['School_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_schools['School_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c92266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'School_Status' where it is 'Closed'\n",
    "combined_schools = combined_schools[combined_schools['School_Status'] != 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop School_Status column\n",
    "combined_schools = combined_schools.drop(columns=['School_Status'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column names to lower casing\n",
    "combined_schools.columns = combined_schools.columns.str.lower()\n",
    "\n",
    "combined_schools.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate school_name\n",
    "duplicate_schools = combined_schools[combined_schools.duplicated(subset=['school_name'])]\n",
    "\n",
    "duplicate_schools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group duplicate_schools by school_name, education_sector, school_type, establishment_year and count the number of occurences\n",
    "duplicate_schools_grouped = duplicate_schools.groupby(['school_name', 'education_sector', 'school_type', 'establishment_year']).size().reset_index(name='count')\n",
    "\n",
    "# get the school_name from duplicate_schools_grouped where count is greater than 1\n",
    "duplicate_school_names = duplicate_schools_grouped[duplicate_schools_grouped['count'] > 1]['school_name']\n",
    "\n",
    "duplicate_school_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_schools_unique = combined_schools[~combined_schools['school_name'].isin(duplicate_school_names)]\n",
    "\n",
    "combined_schools_unique[combined_schools_unique['school_name'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93319bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coburg_special_developmental_school = combined_schools_unique[combined_schools_unique['school_name']==\"Coburg Special Developmental School\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b20079",
   "metadata": {},
   "outputs": [],
   "source": [
    "coburg_special_developmental_school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed665b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_schools_duplicates = combined_schools[combined_schools['school_name'].isin(duplicate_school_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataset\n",
    "output_path = '../data/processed/schools/schools.csv'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "combined_schools.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0c366",
   "metadata": {},
   "source": [
    "**School Rank Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96393bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape this website for top 100 schools in victoria when school type is secondary and 101 for schools that are not in the top 100. If the school type\n",
    "# is not secondry set the column to None\n",
    "# https://bettereducation.com.au/Results/vce.aspx\n",
    "\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-AU,en;q=0.9\",\n",
    "    \"Referer\": \"https://bettereducation.com.au/Results/vce.aspx\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "}\n",
    "url = \"https://bettereducation.com.au/Results/vce.aspx\"\n",
    "resp = requests.get(url, headers=headers, timeout=30)\n",
    "\n",
    "resp.raise_for_status()\n",
    "\n",
    "tables = pd.read_html(io.StringIO(resp.text))  # no match -> gets all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_rank_df = tables[1].copy()\n",
    "# make the Better Education Rank columns go from 1 to 100 including every number in between\n",
    "school_rank_df[\"Better Education Rank\"] = range(1, 101)\n",
    "school_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ed575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import re\n",
    "\n",
    "# Match the schools.csv with the scraped data and add a new column called vic_secondary_rank\n",
    "# If the school is in the top 100, set the rank to the rank from the scraped data\n",
    "# If the school type is secondary or primary/secondary and the school is not in the top 100, set the rank to 101\n",
    "# If the school type is not secondary or primary/secondary, set the rank to None\n",
    "school_rank_df.columns = [c.lower().strip() for c in school_rank_df.columns]\n",
    "school_rank_df = school_rank_df.rename(columns={\"better education rank\": \"vic_secondary_rank\", \"school\": \"school_name\"})\n",
    "school_rank_df[\"school_name\"] = school_rank_df[\"school_name\"].str.strip()\n",
    "school_rank_df = school_rank_df[pd.to_numeric(school_rank_df[\"vic_secondary_rank\"], errors=\"coerce\").notna()]\n",
    "school_rank_df[\"vic_secondary_rank\"] = school_rank_df[\"vic_secondary_rank\"].astype(int) \n",
    "\n",
    "schools = pd.read_csv(\"../data/processed/schools/schools.csv\")\n",
    "def normalize(name: str) -> str:\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    cleaned = (\n",
    "        name.lower()\n",
    "            .replace(\"’\", \"'\")\n",
    "            .replace(\"–\", \"-\")\n",
    "            .strip()\n",
    "    )\n",
    "    # drop everything after the first comma (suburb/campus info)\n",
    "    cleaned = cleaned.split(\",\", 1)[0]\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n",
    "    return cleaned\n",
    "\n",
    "# ranking table\n",
    "school_rank_df[\"name_norm\"] = school_rank_df[\"school_name\"].map(normalize)\n",
    "lookup_norm = dict(zip(school_rank_df[\"name_norm\"], school_rank_df[\"vic_secondary_rank\"]))\n",
    "\n",
    "# schools.csv\n",
    "schools[\"school_name_norm\"] = schools[\"school_name\"].map(normalize)\n",
    "\n",
    "def lookup_fuzzy(name, candidates, cutoff=0.8):\n",
    "    matches = get_close_matches(name, candidates, n=1, cutoff=cutoff)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "candidate_names = list(lookup_norm.keys())\n",
    "\n",
    "def assign_rank(row):\n",
    "    rank = lookup_norm.get(row[\"school_name_norm\"])\n",
    "    if rank is None:\n",
    "        # try fuzzy match against known names (casefolded) and goes both way for better matching\n",
    "        match = lookup_fuzzy(row[\"school_name_norm\"], candidate_names, cutoff=0.85)\n",
    "        if match:\n",
    "            rank = lookup_norm[match]\n",
    "    school_type = row[\"school_type\"].strip().lower()\n",
    "    if rank:\n",
    "        return rank\n",
    "    if school_type not in {\"secondary\", \"pri/sec\"}:\n",
    "        return None\n",
    "    return 101\n",
    "\n",
    "schools[\"vic_secondary_rank\"] = schools.apply(assign_rank, axis=1)\n",
    "schools = schools.drop(columns=[\"school_name_norm\"])\n",
    "schools.to_csv(\"../data/processed/schools/schools_ranked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(range(1, 101)) - set(schools[\"vic_secondary_rank\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools[schools[\"school_name\"].str.contains(\"Haileybury College\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "eps = 0.0001\n",
    "# make a school goodness column based on the vic_secondary_rank\n",
    "def school_goodness(rank):\n",
    "    if pd.isna(rank):\n",
    "        return \"N/A\"\n",
    "    else:\n",
    "        goodness = 1 - (np.log(rank) / (np.log(101)) + eps) # Normalized log rank\n",
    "        return round(goodness, 4)\n",
    "    \n",
    "schools[\"school_goodness\"] = schools[\"vic_secondary_rank\"].map(school_goodness)\n",
    "schools.to_csv(\"../data/processed/schools/schools_ranked.csv\", index=False)\n",
    "schools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e060237",
   "metadata": {},
   "source": [
    "**Best School per Isochrone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4cf2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "# Get the current notebook's directory and go up to project root\n",
    "current_dir = Path().resolve()\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent\n",
    "elif current_dir.name == 'project2':\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    # If we're in the parent directory, look for project2\n",
    "    project_root = current_dir / 'project2'\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f'Project root: {project_root}')\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import Geod\n",
    "from shapely import wkt, ops\n",
    "from utils.preprocess import PreprocessUtils\n",
    "\n",
    "def safe_wkt(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    value = str(value).strip()\n",
    "    if not value:\n",
    "        return None\n",
    "    try:\n",
    "        return wkt.loads(value)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def to_geom(val):\n",
    "    if isinstance(val, str):\n",
    "        cleaned = val.strip()\n",
    "        if cleaned.lower() in {\"\", \"nan\", \"none\"}:\n",
    "            return None\n",
    "        return wkt.loads(cleaned)\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    return val  # already a geometry\n",
    "\n",
    "def clean_geom_cell(val):\n",
    "    if isinstance(val, str) and val.strip().lower() in {\"\", \"nan\", \"none\"}:\n",
    "        return pd.NA\n",
    "    if pd.isna(val):\n",
    "        return pd.NA\n",
    "    return val\n",
    "\n",
    "def swap_axes(geom):\n",
    "    return ops.transform(lambda x, y, z=None: (y, x), geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load + parse ----\n",
    "spark_df_loaded = spark.read.parquet(\"../data/curated/rent_features/cleaned_listings_sampled_parquet\")\n",
    "df_loaded = spark_df_loaded.toPandas()\n",
    "\n",
    "# Convert WKT back to geometry\n",
    "from shapely import wkt\n",
    "df_loaded['coordinates'] = df_loaded['coordinates_wkt'].apply(wkt.loads)\n",
    "df_loaded = df_loaded.drop(columns=['coordinates_wkt'])\n",
    "\n",
    "# Convert back to GeoDataFrame\n",
    "listings_gdf = gpd.GeoDataFrame(df_loaded, geometry='coordinates')\n",
    "\n",
    "iso_columns = [c for c in listings_gdf.columns if c.endswith(\"min_imputed\") or c.endswith(\"min\")]\n",
    "\n",
    "# elementwise across the selected columns\n",
    "listings_gdf[iso_columns] = listings_gdf[iso_columns].map(clean_geom_cell)\n",
    "\n",
    "listings_gdf[\"listing_point\"] = (\n",
    "    listings_gdf[\"coordinates\"]\n",
    "        .apply(safe_wkt)                                  # raw WKT -> Point/None\n",
    "        .apply(lambda g: swap_axes(g) if g is not None else None)\n",
    ")\n",
    "# reset this as the geometry of the gdf\n",
    "# set the GeoDataFrame geometry to the new column\n",
    "listings_gdf = listings_gdf.set_geometry(\"listing_point\")\n",
    "\n",
    "# preserve CRS if you already know it (e.g., EPSG:4326)\n",
    "listings_gdf = listings_gdf.set_crs(\"EPSG:4326\", inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f8959",
   "metadata": {},
   "source": [
    "**Nearest neighbour imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ae989",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreprocessUtils()\n",
    "\n",
    "listings_gdf[['driving_5min_imputed', 'driving_10min_imputed', 'driving_15min_imputed']] = preprocessor.impute_by_nearest_point(listings_gdf, ['driving_5min', 'driving_10min', 'driving_15min'])\n",
    "listings_gdf[['walking_5min_imputed', 'walking_10min_imputed', 'walking_15min_imputed']] = preprocessor.impute_by_nearest_point(listings_gdf, ['walking_5min', 'walking_10min', 'walking_15min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ca9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in iso_columns:\n",
    "    listings_gdf[col] = listings_gdf[col].map(safe_wkt)   \n",
    "\n",
    "for col in iso_columns:\n",
    "    listings_gdf[f\"geom_{col}\"] = listings_gdf[col].apply(to_geom)\n",
    "\n",
    "listings_gdf[\"year\"] = (\n",
    "    listings_gdf[\"year\"]\n",
    "    .astype(\"Int64\")\n",
    ")\n",
    "\n",
    "schools[\"establishment_year\"] = (\n",
    "    pd.to_numeric(schools[\"establishment_year\"], errors=\"coerce\")\n",
    "      .round()\n",
    "      .astype(\"Int64\")   # null-friendly\n",
    ")\n",
    "schools[\"geometry\"] = schools[\"coordinates\"].apply(safe_wkt)\n",
    "schools[\"coordinates\"] = schools[\"coordinates\"].apply(safe_wkt)\n",
    "schools_gdf = gpd.GeoDataFrame(schools, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "beta = 0.2 # equivalent to lambda = 1/ beta \n",
    "def score_row(goodness, dist_km):\n",
    "    return goodness / (1 + beta * dist_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828abe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a57132",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {}\n",
    "for col in iso_columns:\n",
    "    if col.endswith(\"_imputed\"):\n",
    "        base = col.replace(\"_imputed\", \"\")\n",
    "        if base in listings_gdf.columns:\n",
    "            pairs[base] = (base, col)\n",
    "\n",
    "missing_pairs = pd.Series(False, index=listings_gdf.index)\n",
    "\n",
    "for base, (orig, imp) in pairs.items():\n",
    "    both_null = listings_gdf[[orig, imp]].isna().all(axis=1)\n",
    "    if both_null.any():\n",
    "        print(f\"{base}: {both_null.sum()} rows missing both original and imputed\")\n",
    "        missing_pairs |= both_null\n",
    "\n",
    "rows_missing_any_pair = listings_gdf.loc[missing_pairs]\n",
    "print(\"Rows missing every column in at least one pair:\", len(rows_missing_any_pair))\n",
    "\n",
    "# Adding a flag feature for listings with both original and imputed missing\n",
    "\n",
    "pairs = [\n",
    "    (base, f\"{base}_imputed\")\n",
    "    for base in iso_columns\n",
    "    if base in listings_gdf.columns and f\"{base}_imputed\" in listings_gdf.columns\n",
    "]\n",
    "\n",
    "missing_iso = pd.Series(False, index=listings_gdf.index)\n",
    "for base, imp in pairs:\n",
    "    missing_iso |= listings_gdf[[base, imp]].isna().all(axis=1)\n",
    "\n",
    "listings_gdf[\"missing_iso_feature\"] = missing_iso.astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeda5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cols = [c for c in listings_gdf.columns if c.startswith(\"geom_\") and \"_imputed\" not in c]\n",
    "iso_columns_imp = [c for c in iso_columns if \"_imputed\" not in c]\n",
    "# imputes the isochrone and the geom column with nearest neighbour if they are empty\n",
    "for col in best_cols:\n",
    "    imp = f\"{col}_imputed\"\n",
    "    if imp in listings_gdf.columns:\n",
    "        listings_gdf[col] = listings_gdf[col].fillna(listings_gdf[imp]).infer_objects(copy=False)\n",
    "\n",
    "for col in iso_columns_imp:\n",
    "    imp = f\"{col}_imputed\"\n",
    "    if imp in listings_gdf.columns:\n",
    "        listings_gdf[col] = listings_gdf[col].fillna(listings_gdf[imp]).infer_objects(copy=False)\n",
    "\n",
    "\n",
    "to_drop1 = [c for c in listings_gdf.columns if c.endswith(\"min_imputed\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_gdf = listings_gdf.drop(columns=to_drop1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = listings_gdf.iloc[:, -15:]      # grab the trailing 30 columns\n",
    "mask = subset.isna().all(axis=1)         # True where all 30 are null/NaN\n",
    "count_empty = mask.sum()                 # how many such rows\n",
    "\n",
    "empty_rows = listings_gdf.loc[mask]      # inspect the actual rows\n",
    "print(\"Rows with all of the last 30 columns empty:\", count_empty)\n",
    "empty_rows.head()                                # or empty_rows[subset.columns] to focus on that slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_columns2 = [c for c in listings_gdf.columns if c.endswith(\"min\") and \"geom_\" not in c]\n",
    "\n",
    "iso_columns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b129695",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "iso_columns2 = [c for c in listings_gdf.columns if c.endswith(\"min\") and \"geom_\" not in c]\n",
    "\n",
    "for col in iso_columns2:\n",
    "    poly_col = f\"geom_{col}\"\n",
    "    # activate polygon geometry and drop rows without polygons\n",
    "    iso_poly = listings_gdf.set_geometry(poly_col)\n",
    "    iso_poly = iso_poly[iso_poly[poly_col].notna()]\n",
    "    iso_poly = iso_poly.set_crs(\"EPSG:4326\", allow_override=True)  # define if missing\n",
    "    # (optionally ensure validity)\n",
    "    # iso_poly[poly_col] = iso_poly[poly_col].buffer(0)\n",
    "\n",
    "    # spatial join: schools inside polygon (keep listing_point column intact)\n",
    "    joined = gpd.sjoin(\n",
    "        iso_poly,\n",
    "        schools_gdf,\n",
    "        how=\"left\",\n",
    "        predicate=\"covers\",\n",
    "        rsuffix=\"school\",\n",
    "    )\n",
    "\n",
    "    # only keep schools that existed by first_listed_year (or unknown year)\n",
    "    mask = joined[\"establishment_year\"].isna() | (\n",
    "        joined[\"establishment_year\"] <= joined[\"year\"]\n",
    "    )\n",
    "    joined = joined[mask]\n",
    "\n",
    "    if not joined.empty:\n",
    "        # geodesic distance from listing point to school (FIX 3 & 4)\n",
    "        lon1 = joined[\"listing_point\"].apply(lambda g: g.x).values\n",
    "        lat1 = joined[\"listing_point\"].apply(lambda g: g.y).values\n",
    "        lon2 = gpd.GeoSeries(joined[\"coordinates_school\"]).x\n",
    "        lat2 = gpd.GeoSeries(joined[\"coordinates_school\"]).y\n",
    "        _, _, dists_m = geod.inv(lon1, lat1, lon2, lat2)\n",
    "        joined[\"dist_km\"] = dists_m / 1000.0\n",
    "    \n",
    "        joined[\"school_goodness\"] = pd.to_numeric(joined[\"school_goodness\"], errors=\"coerce\")\n",
    "        joined[\"dist_km\"] = pd.to_numeric(joined[\"dist_km\"], errors=\"coerce\")\n",
    "\n",
    "        valid = joined[\"school_goodness\"].notna() & joined[\"dist_km\"].notna()\n",
    "        joined[\"score\"] = pd.NA\n",
    "        joined.loc[valid, \"score\"] = score_row(joined.loc[valid, \"school_goodness\"], joined.loc[valid, \"dist_km\"])\n",
    "\n",
    "        # a count of how many schools within a given isochrone\n",
    "        count = (joined.groupby(\"property_id\")[\"school_name\"]\n",
    "                .count()                                 # counts rows in each group\n",
    "                .rename(f\"n_schools_{col}\")             # e.g., n_schools_driving_5min\n",
    "                .to_frame())\n",
    "\n",
    "        best_inside = (\n",
    "            joined.dropna(subset=[\"score\"])\n",
    "            .sort_values(\"score\", ascending=False)\n",
    "            .groupby(\"property_id\")\n",
    "            .head(1)\n",
    "            .set_index(\"property_id\")\n",
    "        )\n",
    "\n",
    "        # build out indexed by property_id\n",
    "        out = (iso_poly[[\"property_id\"]]\n",
    "            .drop_duplicates()\n",
    "            .set_index(\"property_id\"))\n",
    "        for c in [f\"best_school_name_{col}\", f\"best_school_coord_{col}\", f\"best_score_{col}\", f\"best_dist_km_{col}\"]:\n",
    "            out[c] = None\n",
    "\n",
    "        if not best_inside.empty:\n",
    "            idx = best_inside.index\n",
    "            out.loc[idx, f\"best_school_name_{col}\"]  = best_inside[\"school_name\"].to_numpy()\n",
    "            out.loc[idx, f\"best_school_coord_{col}\"] = best_inside[\"coordinates_school\"].to_numpy()\n",
    "            out.loc[idx, f\"best_score_{col}\"]        = best_inside[\"score\"].to_numpy()\n",
    "            out.loc[idx, f\"best_dist_km_{col}\"]      = best_inside[\"dist_km\"].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "        # join back to listings_gdf by property_id\n",
    "        all_results.append(out)\n",
    "        all_results.append(count)\n",
    "\n",
    "final_out = pd.concat(all_results, axis=1)  # columns are already unique per {col}\n",
    "overlap = final_out.columns.intersection(listings_gdf.columns)\n",
    "listings_gdf = (\n",
    "    listings_gdf\n",
    "    .drop(columns=overlap, errors=\"ignore\")\n",
    "    .join(final_out, on=\"property_id\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9cbae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896f949",
   "metadata": {},
   "source": [
    "**Filling in Walking and Driving best columns with nearest neighbour column logic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37946781",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"best_school_name\", \"best_school_coord\", \"best_score\", \"best_dist_km\"]\n",
    "\n",
    "def all_fields_present(df, token):\n",
    "    mode, dur = token.split(\"_\", 1)\n",
    "    cols = [f\"{f}_{mode}_{dur}\" for f in fields if f\"{f}_{mode}_{dur}\" in df.columns]\n",
    "    if not cols:\n",
    "        return pd.Series(False, index=df.index)\n",
    "    return df[cols].notna().all(axis=1)\n",
    "\n",
    "# availability for every token we might touch\n",
    "tokens = [\n",
    "    \"walking_5min\", \"walking_10min\", \"walking_15min\",\n",
    "    \"driving_5min\", \"driving_10min\", \"driving_15min\",\n",
    "]\n",
    "avail = {tok: all_fields_present(listings_gdf, tok) for tok in tokens}\n",
    "\n",
    "# per-target fallback order\n",
    "fallback = {\n",
    "    \"walking_5min\":  [\"walking_10min\", \"walking_15min\", \"driving_5min\", \"driving_10min\", \"driving_15min\"],\n",
    "    \"walking_10min\": [\"walking_15min\", \"driving_5min\", \"driving_10min\", \"driving_15min\"],\n",
    "    \"walking_15min\": [\"driving_5min\", \"driving_10min\", \"driving_15min\"],\n",
    "    \"driving_5min\":  [\"driving_10min\", \"driving_15min\"],\n",
    "    \"driving_10min\": [\"driving_5min\", \"driving_15min\"],\n",
    "    \"driving_15min\": [\"driving_10min\", \"driving_5min\"],\n",
    "}\n",
    "\n",
    "for target in [\"walking_5min\", \"walking_10min\", \"driving_5min\", \"driving_10min\", \"driving_15min\"]:\n",
    "    mode_tgt, dur_tgt = target.split(\"_\", 1)\n",
    "    target_cols = [f\"{f}_{mode_tgt}_{dur_tgt}\" for f in fields if f\"{f}_{mode_tgt}_{dur_tgt}\" in listings_gdf.columns]\n",
    "    if not target_cols:\n",
    "        continue\n",
    "\n",
    "    missing = listings_gdf[target_cols].isna().all(axis=1)\n",
    "    filled = pd.Series(False, index=listings_gdf.index)\n",
    "\n",
    "    for candidate in fallback.get(target, []):\n",
    "        rows = missing & avail[candidate] & ~filled\n",
    "        if not rows.any():\n",
    "            continue\n",
    "\n",
    "        mode_src, dur_src = candidate.split(\"_\", 1)\n",
    "        for f in fields:\n",
    "            src_col = f\"{f}_{mode_src}_{dur_src}\"\n",
    "            tgt_col = f\"{f}_{mode_tgt}_{dur_tgt}\"\n",
    "            if src_col in listings_gdf.columns and tgt_col in listings_gdf.columns:\n",
    "                listings_gdf.loc[rows, tgt_col] = listings_gdf.loc[rows, src_col].values\n",
    "\n",
    "        tgt_cnt = f\"n_schools_{mode_tgt}_{dur_tgt}\"\n",
    "        src_cnt = f\"n_schools_{mode_src}_{dur_src}\"\n",
    "        if tgt_cnt in listings_gdf.columns and src_cnt in listings_gdf.columns:\n",
    "            need = rows & listings_gdf[tgt_cnt].fillna(0).eq(0)\n",
    "            listings_gdf.loc[need, tgt_cnt] = listings_gdf.loc[need, src_cnt].values\n",
    "\n",
    "        filled |= rows  # stop once we copy from the first available candidate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [c for c in listings_gdf.columns if c.startswith(\"best_score_\")]\n",
    "listings_gdf[score_cols] = listings_gdf[score_cols].fillna(3e-8) # Justification can be asked by venura(formula with reasonably max args)\n",
    "dist_cols = [c for c in listings_gdf.columns if c.startswith(\"best_dist_km_\")]\n",
    "for col in dist_cols:\n",
    "    max_val = listings_gdf[col].max(skipna=True)\n",
    "    listings_gdf[col] = listings_gdf[col].fillna(max_val + 1 if pd.notna(max_val) else 1.0)\n",
    "\n",
    "count_cols = [c for c in listings_gdf.columns if c.startswith(\"n_schools_\")]\n",
    "listings_gdf[count_cols] = listings_gdf[count_cols].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = listings_gdf.iloc[:, -18:].isna().any(axis=1)\n",
    "\n",
    "# should be zero as all missing values are imputed\n",
    "len(listings_gdf.loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to a csv file\n",
    "listings_gdf.to_csv(\"../data/curated/rent_features/cleaned_listings_isochrones_added_with_best_schools.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_gdf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

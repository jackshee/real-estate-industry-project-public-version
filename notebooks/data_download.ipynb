{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5460dfe3",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "Begin with downloading the provided historic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f895fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from urllib.request import urlretrieve\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b93113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_folder(output_dir):\n",
    "    \"\"\"\n",
    "    Create folders for each stage of the ETL pipeline\n",
    "    :param output_dir: The base directory where the folders will be created\n",
    "    \"\"\"\n",
    "    # set output directory\n",
    "\n",
    "    \n",
    "    # check if data directory exists, if not create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # create folders for each stage of the ETL pipeline\n",
    "    for stage in ['landing', 'raw', 'curated', 'analysis']:\n",
    "        stage_path = os.path.join(output_dir, stage)\n",
    "        if not os.path.exists(stage_path):\n",
    "            os.makedirs(stage_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9bea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, output_path, file_type):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a specified output path\n",
    "    :param url: The URL of the file to download\n",
    "    :param output_path: The local path where the file will be saved\n",
    "    :param file_type: The file extension/type (e.g., 'csv', 'json', 'xlsx')\n",
    "    \"\"\"\n",
    "    # generate output file path\n",
    "    output_file_path = f\"{output_path}.{file_type}\"\n",
    "\n",
    "    # check if output file already exists\n",
    "    if not os.path.exists(output_file_path):\n",
    "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "        # download the file from the URL and save it to the output file path\n",
    "        urlretrieve(url, output_file_path)\n",
    "        print(f\"File downloaded and saved to {output_file_path}\")\n",
    "    else:\n",
    "        print(f\"File already exists at {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135eb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data directories\n",
    "create_data_folder('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9d01c",
   "metadata": {},
   "source": [
    "**Download moving annual rent by suburb from ABS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d972787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rent_by_suburb directory\n",
    "directory = '../data/landing/rent/rent_by_suburb'\n",
    "\n",
    "# URL\n",
    "URL_TEMPLATE = \"https://www.dffh.vic.gov.au/moving-annual-rents-suburb-march-quarter-2023-excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba3f57f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ../data/landing/rent/rent_by_suburb.xlsx\n"
     ]
    }
   ],
   "source": [
    "download_file(URL_TEMPLATE, directory, 'xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa08302",
   "metadata": {},
   "source": [
    "**Download Public Transport Lines and Stops from VIC Gov open data (public transport)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87544a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create public_transport_stops directory\n",
    "directory = '../data/landing/ptv/public_transport_stops'\n",
    "\n",
    "# URL\n",
    "URL_TEMPLATE = \"https://opendata.transport.vic.gov.au/dataset/6d36dfd9-8693-4552-8a03-05eb29a391fd/resource/afa7b823-0c8b-47a1-bc40-ada565f684c7/download/public_transport_stops.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8767b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ../data/landing/ptv/public_transport_stops.geojson\n"
     ]
    }
   ],
   "source": [
    "download_file(URL_TEMPLATE, directory, 'geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4dae56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create public_transport_lines directory\n",
    "directory = '../data/landing/ptv/public_transport_lines'\n",
    "\n",
    "# URL\n",
    "URL_TEMPLATE = \"https://opendata.transport.vic.gov.au/dataset/6d36dfd9-8693-4552-8a03-05eb29a391fd/resource/52e5173e-b5d5-4b65-9b98-89f225fc529c/download/public_transport_lines.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d081f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ../data/landing/ptv/public_transport_lines.geojson\n"
     ]
    }
   ],
   "source": [
    "download_file(URL_TEMPLATE, directory, 'geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee9819",
   "metadata": {},
   "source": [
    "**Download School Locations Data**\n",
    "\n",
    "We will download school locations from 2023 to 2025. \\\n",
    "There will be one dataset for each of the years we scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "782f06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "schools_23 = \"https://www.education.vic.gov.au/Documents/about/research/datavic/dv346-schoollocations2023.csv\"\n",
    "schools_24 = \"https://www.education.vic.gov.au/Documents/about/research/datavic/dv378_DataVic-SchoolLocations-2024.csv\"\n",
    "schools_25 = \"https://www.education.vic.gov.au/Documents/about/research/datavic/dv402-SchoolLocations2025.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00a37213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ../data/landing/schools/school_locations_2023.csv\n",
      "File downloaded and saved to ../data/landing/schools/school_locations_2024.csv\n",
      "File downloaded and saved to ../data/landing/schools/school_locations_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# create 2023 school locations directory\n",
    "directory = '../data/landing/schools/school_locations_2023'\n",
    "download_file(schools_23, directory, 'csv')\n",
    "# create 2024 school locations directory\n",
    "directory = '../data/landing/schools/school_locations_2024'\n",
    "download_file(schools_24, directory, 'csv')\n",
    "# create 2025 school locations directory\n",
    "directory = '../data/landing/schools/school_locations_2025'\n",
    "download_file(schools_25, directory, 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6ac4a",
   "metadata": {},
   "source": [
    "**Download Open Space Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17f65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create open_space directory\n",
    "directory = '../data/landing/open_space/open_space'\n",
    "\n",
    "# URL\n",
    "URL_TEMPLATE = \"https://opendata.arcgis.com/datasets/da1c06e3ab6948fcb56de4bb3c722449_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "893a68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ../data/landing/open_space/open_space.csv\n"
     ]
    }
   ],
   "source": [
    "download_file(URL_TEMPLATE, directory, 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a45011",
   "metadata": {},
   "source": [
    "**Download Moving Annual Rent by Suburb from DFFH (Latest File)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ae9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading latest moving annual rent file: moving_annual_median_weekly_rent_by_suburb\n",
      "URL: https://www.dffh.vic.gov.au/moving-annual-rent-suburb-march-quarter-2025-excel\n"
     ]
    }
   ],
   "source": [
    "# Download the latest moving annual rent file (March 2025)\n",
    "# This file contains all historical data from previous quarters and years\n",
    "latest_url = \"https://www.dffh.vic.gov.au/moving-annual-rent-suburb-march-quarter-2025-excel\"\n",
    "filename = \"moving_annual_median_weekly_rent_by_suburb\"\n",
    "\n",
    "print(f\"Downloading latest moving annual rent file: {filename}\")\n",
    "print(f\"URL: {latest_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a8618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to ../data/landing/moving_annual_rent/moving_annual_median_weekly_rent_by_suburb.xlsx\n",
      "Successfully downloaded latest moving annual rent file!\n"
     ]
    }
   ],
   "source": [
    "# Download the latest moving annual rent file\n",
    "directory = f'../data/landing/moving_annual_rent/{filename}'\n",
    "try:\n",
    "    download_file(latest_url, directory, 'xlsx')\n",
    "    print(\"Successfully downloaded latest moving annual rent file!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9410e",
   "metadata": {},
   "source": [
    "**Download Victorian Unemployment Rate Data**\n",
    "\n",
    "Scrape monthly unemployment rate data from the Victorian labour market website and aggregate by quarter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a7e151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping unemployment data from Victorian labour market website...\n",
      "Successfully scraped and saved unemployment data to ../data/landing/unemployment_rate/quarterly_unemployment_rate.csv\n",
      "Data contains 191 quarterly records\n",
      "Date range: 1978-03-01 00:00:00 to 2025-09-01 00:00:00\n",
      "\n",
      "First 5 records:\n",
      "        date  year  quarter  avg_unemployment_rate\n",
      "0 1978-03-01  1978        1               5.842250\n",
      "1 1978-06-01  1978        2               5.552600\n",
      "2 1978-09-01  1978        3               5.550233\n",
      "3 1978-12-01  1978        4               5.575700\n",
      "4 1979-03-01  1979        1               5.600167\n",
      "\n",
      "Last 5 records:\n",
      "          date  year  quarter  avg_unemployment_rate\n",
      "186 2024-09-01  2024        3               4.473833\n",
      "187 2024-12-01  2024        4               4.378333\n",
      "188 2025-03-01  2025        1               4.544433\n",
      "189 2025-06-01  2025        2               4.381900\n",
      "190 2025-09-01  2025        3               4.472050\n"
     ]
    }
   ],
   "source": [
    "def scrape_unemployment_data():\n",
    "    \"\"\"\n",
    "    Scrape unemployment data from the Victorian labour market website\n",
    "    and aggregate by quarter taking the average unemployment rate\n",
    "    \"\"\"\n",
    "    url = \"https://djsir-data.github.io/djprecodash/tables/djsir_labour_market\"\n",
    "    \n",
    "    try:\n",
    "        # Make request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the table containing unemployment data\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            raise ValueError(\"No table found on the webpage\")\n",
    "        \n",
    "        # Extract table data\n",
    "        rows = table.find_all('tr')\n",
    "        data = []\n",
    "        \n",
    "        for row in rows[1:]:  # Skip header row\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 2:\n",
    "                date_str = cells[0].get_text(strip=True)\n",
    "                unemployment_rate = cells[1].get_text(strip=True)\n",
    "                \n",
    "                # Skip rows with missing data\n",
    "                if date_str and unemployment_rate and unemployment_rate != '':\n",
    "                    data.append({\n",
    "                        'date': date_str,\n",
    "                        'unemployment_rate': float(unemployment_rate)\n",
    "                    })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(\"No data found in the table\")\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Create year and quarter columns\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        \n",
    "        # Group by year and quarter and calculate average unemployment rate\n",
    "        quarterly_data = df.groupby(['year', 'quarter'])['unemployment_rate'].mean().reset_index()\n",
    "        \n",
    "        # Create a proper date column for quarters\n",
    "        quarterly_data['quarter_date'] = pd.to_datetime(\n",
    "            quarterly_data['year'].astype(str) + '-' + \n",
    "            (quarterly_data['quarter'] * 3).astype(str) + '-01'\n",
    "        )\n",
    "        \n",
    "        # Sort by date\n",
    "        quarterly_data = quarterly_data.sort_values('quarter_date')\n",
    "        \n",
    "        # Select relevant columns for final output\n",
    "        final_data = quarterly_data[['quarter_date', 'year', 'quarter', 'unemployment_rate']].copy()\n",
    "        final_data.columns = ['date', 'year', 'quarter', 'avg_unemployment_rate']\n",
    "        \n",
    "        return final_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping unemployment data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create unemployment rate directory\n",
    "unemployment_dir = '../data/landing/unemployment_rate'\n",
    "os.makedirs(unemployment_dir, exist_ok=True)\n",
    "\n",
    "print(\"Scraping unemployment data from Victorian labour market website...\")\n",
    "unemployment_df = scrape_unemployment_data()\n",
    "\n",
    "if unemployment_df is not None:\n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(unemployment_dir, 'quarterly_unemployment_rate.csv')\n",
    "    unemployment_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Successfully scraped and saved unemployment data to {output_path}\")\n",
    "    print(f\"Data contains {len(unemployment_df)} quarterly records\")\n",
    "    print(f\"Date range: {unemployment_df['date'].min()} to {unemployment_df['date'].max()}\")\n",
    "    print(\"\\nFirst 5 records:\")\n",
    "    print(unemployment_df.head())\n",
    "    print(\"\\nLast 5 records:\")\n",
    "    print(unemployment_df.tail())\n",
    "else:\n",
    "    print(\"Failed to scrape unemployment data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4486dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_time_series_data(url, data_name, value_columns=None, aggregate_method='mean'):\n",
    "    \"\"\"\n",
    "    General function to scrape time series data from Victorian government tables\n",
    "    and aggregate by quarter\n",
    "    \n",
    "    Parameters:\n",
    "    - url: URL of the webpage containing the time series table\n",
    "    - data_name: Name for the dataset (used for output file naming)\n",
    "    - value_columns: List of column indices to extract (default: all columns after date)\n",
    "    - aggregate_method: Method for aggregation ('mean', 'last', 'first')\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with quarterly aggregated data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the table containing data\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            raise ValueError(\"No table found on the webpage\")\n",
    "        \n",
    "        # Extract header row to understand column structure\n",
    "        header_row = table.find('tr')\n",
    "        headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]\n",
    "        \n",
    "        # If no value_columns specified, use all columns after the first (date) column\n",
    "        if value_columns is None:\n",
    "            value_columns = list(range(1, len(headers)))\n",
    "        \n",
    "        # Extract table data\n",
    "        rows = table.find_all('tr')[1:]  # Skip header row\n",
    "        all_data = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) > 0:\n",
    "                date_str = cells[0].get_text(strip=True)\n",
    "                \n",
    "                # Skip rows with missing date\n",
    "                if not date_str:\n",
    "                    continue\n",
    "                \n",
    "                row_data = {'date': date_str}\n",
    "                \n",
    "                # Extract specified columns\n",
    "                for col_idx in value_columns:\n",
    "                    if col_idx < len(cells):\n",
    "                        value_str = cells[col_idx].get_text(strip=True)\n",
    "                        col_name = headers[col_idx] if col_idx < len(headers) else f'value_{col_idx}'\n",
    "                        \n",
    "                        # Handle missing values\n",
    "                        if value_str and value_str.strip() != '':\n",
    "                            try:\n",
    "                                row_data[col_name] = float(value_str)\n",
    "                            except ValueError:\n",
    "                                # Skip rows with non-numeric values\n",
    "                                row_data = None\n",
    "                                break\n",
    "                        else:\n",
    "                            row_data[col_name] = None\n",
    "                \n",
    "                if row_data is not None:\n",
    "                    all_data.append(row_data)\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No valid data found in the table\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Create year and quarter columns\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        \n",
    "        # Group by year and quarter and aggregate\n",
    "        agg_dict = {}\n",
    "        for col in df.columns:\n",
    "            if col not in ['date', 'year', 'quarter']:\n",
    "                if aggregate_method == 'mean':\n",
    "                    agg_dict[col] = 'mean'\n",
    "                elif aggregate_method == 'last':\n",
    "                    agg_dict[col] = 'last'\n",
    "                elif aggregate_method == 'first':\n",
    "                    agg_dict[col] = 'first'\n",
    "        \n",
    "        quarterly_data = df.groupby(['year', 'quarter']).agg(agg_dict).reset_index()\n",
    "        \n",
    "        # Create a proper date column for quarters\n",
    "        quarterly_data['quarter_date'] = pd.to_datetime(\n",
    "            quarterly_data['year'].astype(str) + '-' + \n",
    "            (quarterly_data['quarter'] * 3).astype(str) + '-01'\n",
    "        )\n",
    "        \n",
    "        # Sort by date\n",
    "        quarterly_data = quarterly_data.sort_values('quarter_date')\n",
    "        \n",
    "        # Prepare final output\n",
    "        final_columns = ['quarter_date', 'year', 'quarter']\n",
    "        for col in quarterly_data.columns:\n",
    "            if col not in ['quarter_date', 'year', 'quarter']:\n",
    "                final_columns.append(col)\n",
    "        \n",
    "        final_data = quarterly_data[final_columns].copy()\n",
    "        final_data.rename(columns={'quarter_date': 'date'}, inplace=True)\n",
    "        \n",
    "        return final_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {data_name} data: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_time_series_data(data, data_name, output_dir):\n",
    "    \"\"\"\n",
    "    Save time series data to CSV with appropriate naming\n",
    "    \"\"\"\n",
    "    if data is not None:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_path = os.path.join(output_dir, f'quarterly_{data_name}.csv')\n",
    "        data.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"Successfully scraped and saved {data_name} data to {output_path}\")\n",
    "        print(f\"Data contains {len(data)} quarterly records\")\n",
    "        print(f\"Date range: {data['date'].min()} to {data['date'].max()}\")\n",
    "        print(\"\\nFirst 5 records:\")\n",
    "        print(data.head())\n",
    "        print(\"\\nLast 5 records:\")\n",
    "        print(data.tail())\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return output_path\n",
    "    else:\n",
    "        print(f\"Failed to scrape {data_name} data\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape unemployment rate data\n",
    "print(\"=== SCRAPING UNEMPLOYMENT RATE DATA ===\")\n",
    "unemployment_url = \"https://djsir-data.github.io/djprecodash/tables/djsir_labour_market\"\n",
    "unemployment_data = scrape_time_series_data(\n",
    "    url=unemployment_url,\n",
    "    data_name=\"unemployment_rate\",\n",
    "    value_columns=[1],  # Only unemployment rate column\n",
    "    aggregate_method='mean'\n",
    ")\n",
    "unemployment_output = save_time_series_data(\n",
    "    unemployment_data, \n",
    "    \"unemployment_rate\", \n",
    "    \"../data/landing/unemployment_rate\"\n",
    ")\n",
    "\n",
    "# Scrape interest rates data\n",
    "print(\"=== SCRAPING INTEREST RATES DATA ===\")\n",
    "interest_rates_url = \"https://djsir-data.github.io/djprecodash/tables/djsir_interest_rates\"\n",
    "interest_rates_data = scrape_time_series_data(\n",
    "    url=interest_rates_url,\n",
    "    data_name=\"interest_rates\",\n",
    "    value_columns=[1, 2, 3],  # Mortgage rates, Savings rates, Cash rate\n",
    "    aggregate_method='mean'\n",
    ")\n",
    "interest_rates_output = save_time_series_data(\n",
    "    interest_rates_data, \n",
    "    \"interest_rates\", \n",
    "    \"../data/landing/interest_rates\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbb3ec",
   "metadata": {},
   "source": [
    "**Summary of Downloaded Economic Data**\n",
    "\n",
    "The script has successfully scraped and processed the following datasets:\n",
    "\n",
    "1. **Unemployment Rate Data**: Monthly unemployment rates from 1978-2025, aggregated to quarterly averages\n",
    "   - Source: [Victorian Labour Market](https://djsir-data.github.io/djprecodash/tables/djsir_labour_market)\n",
    "   - Output: `data/landing/unemployment_rate/quarterly_unemployment_rate.csv`\n",
    "\n",
    "2. **Interest Rates Data**: Monthly interest rates (Mortgage, Savings, Cash rates) from 1990-2035, aggregated to quarterly averages\n",
    "   - Source: [Victorian Interest Rates](https://djsir-data.github.io/djprecodash/tables/djsir_interest_rates)\n",
    "   - Output: `data/landing/interest_rates/quarterly_interest_rates.csv`\n",
    "\n",
    "Both datasets are now ready for analysis with consistent quarterly time series format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd0ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

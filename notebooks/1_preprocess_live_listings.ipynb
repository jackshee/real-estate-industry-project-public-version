{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a8623b",
   "metadata": {},
   "source": [
    "This notebook performs preprocessing on the scraped listings from the live version of the Domain website with extra features due to the presence of individual property pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9ec842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a21576c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "from utils.preprocess import PreprocessUtils\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = PreprocessUtils()\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  # Show all rows, default is 10\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns, default is 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288567c5",
   "metadata": {},
   "source": [
    "Permitted property types are defined as \"renter friendly types\". \n",
    "\n",
    "The following will be mapped to **House**\n",
    "- House\n",
    "- Townhouse\n",
    "- Villa\n",
    "- New House & Land\n",
    "- Semi-Detached\n",
    "- Terrace\n",
    "- Duplex\n",
    "\n",
    "The following will be mapped to **Flat**\n",
    "- Apartment / Unit / Flat\n",
    "- Studio\n",
    "- New Apartments / Off the Plan\n",
    "- Penthouse\n",
    "\n",
    "The following will be dropped\n",
    "- Vacant Land\n",
    "- Carspace\n",
    "- Block of Units\n",
    "- Acreage / Semi-rural\n",
    "- Rural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "055a927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: data shape is  (14146, 47)\n",
      "After filtering: data shape is  (14077, 43)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/raw/domain/rental_listings_2025_09.csv\")\n",
    "\n",
    "# convert column names to lowercase with snake case\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "print(\"Before filtering: data shape is \", df.shape)\n",
    "# remove rows where property_id is null\n",
    "df = df[df['property_id'].notna()]\n",
    "\n",
    "# remove rows where property_features is null\n",
    "df = df[df['property_features'].notna()]\n",
    "\n",
    "# drop the bathrooms, bedrooms, car_spaces, land_area columns \n",
    "df = df.drop(columns=['bathrooms', 'bedrooms', 'car_spaces', 'land_area'])\n",
    "\n",
    "# drop rows with not permitted property_type\n",
    "permitted_types = [\"house\", \"new house & land\", \"townhouse\", \"villa\", \"semi-detached\", \"terrace\", \"duplex\",\n",
    "                   \"apartment / unit / flat\", \"studio\", \"new apartments / off the plan\", \"penthouse\",\n",
    "                   ]\n",
    "# remove rows where description is null\n",
    "df = df[df['description'].notna()]\n",
    "\n",
    "df['property_type'] = df['property_type'].str.lower()\n",
    "df = df[df['property_type'].isin(permitted_types)]\n",
    "\n",
    "print(\"After filtering: data shape is \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0ca0ea21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "house_flat_other\n",
       "house      8725\n",
       "unknown    5115\n",
       "flat        237\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"house_flat_other\"] = preprocessor.map_property_type(df['property_type'])\n",
    "\n",
    "df[\"house_flat_other\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f9c72110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the suburbs to standardized names (used by DFFH dataset)\n",
    "df['suburb'] = preprocessor.map_suburb(df['suburb'])\n",
    "\n",
    "# Remove the suburbs that have count less than 10\n",
    "suburb_counts = df['suburb'].value_counts()\n",
    "valid_suburbs = suburb_counts[suburb_counts > 10].index\n",
    "df = df[df['suburb'].isin(valid_suburbs)]\n",
    "\n",
    "df['suburb'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfb628",
   "metadata": {},
   "source": [
    "Because the live Domain website scraped the property page for `property_features` the format is slightly different to the format for the listings scraped from Wayback archived summary listings. Here we convert `property_features` back into `bedrooms`, `bathrooms`, `car_spaces` features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21b71cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_property_features(feature_string):\n",
    "    \"\"\"\n",
    "    Parse property_features column to extract bedrooms, bathrooms, car_spaces, and land_area.\n",
    "    \n",
    "    Format: 'bedrooms, ,bathrooms, ,car_spaces,' or 'bedrooms, ,bathrooms, ,car_spaces, ,XXXm²,'\n",
    "            or 'bedrooms, ,bathrooms, ,car_spaces, ,X.XXha,'\n",
    "    Missing values are represented by '−'\n",
    "    Land area can be in m² or ha (hectares are converted to m²: 1 ha = 10,000 m²)\n",
    "    \n",
    "    Returns: pd.Series with four integer values (bedrooms, bathrooms, car_spaces, land_area)\n",
    "    \"\"\"    \n",
    "    # Split by ', ,'\n",
    "    parts = feature_string.split(', ,')\n",
    "    \n",
    "    # Initialize values\n",
    "    bedrooms = None\n",
    "    bathrooms = None\n",
    "    car_spaces = None\n",
    "    land_area = None\n",
    "    \n",
    "    # Extract bedrooms (index 0)\n",
    "    if len(parts) > 0:\n",
    "        val = parts[0].strip().rstrip(',')\n",
    "        # Check if this is just a land area value (like '12.51ha,')\n",
    "        if 'ha' in val or 'm²' in val:\n",
    "            # This entire string is just land area, extract it\n",
    "            if 'ha' in val:\n",
    "                land_area_str = val.replace('ha', '').replace(',', '').strip()\n",
    "                if land_area_str and land_area_str != '−':\n",
    "                    land_area = int(float(land_area_str) * 10000)  # Convert ha to m²\n",
    "            elif 'm²' in val:\n",
    "                land_area_str = val.replace('m²', '').replace(',', '').strip()\n",
    "                if land_area_str and land_area_str != '−':\n",
    "                    land_area = int(land_area_str)\n",
    "        else:\n",
    "            bedrooms = None if val == '−' or val == '' else int(val)\n",
    "    \n",
    "    # Extract bathrooms (index 1)\n",
    "    if len(parts) > 1:\n",
    "        val = parts[1].strip().rstrip(',')\n",
    "        bathrooms = None if val == '−' or val == '' else int(val)\n",
    "    \n",
    "    # Extract car_spaces (index 2)\n",
    "    if len(parts) > 2:\n",
    "        val = parts[2].strip().rstrip(',')\n",
    "        car_spaces = None if val == '−' or val == '' else int(val)\n",
    "    \n",
    "    # Extract land_area (index 3, if present and not already extracted)\n",
    "    if land_area is None and len(parts) > 3:\n",
    "        val = parts[3].strip().rstrip(',')\n",
    "        if 'ha' in val:\n",
    "            # Remove 'ha' and extract number (handle commas and decimals like '12.51ha')\n",
    "            land_area_str = val.replace('ha', '').replace(',', '').strip()\n",
    "            if land_area_str and land_area_str != '−':\n",
    "                land_area = int(float(land_area_str) * 10000)  # Convert ha to m², then to int\n",
    "        elif 'm²' in val:\n",
    "            # Remove 'm²' and extract number (handle commas in numbers like '5,030m²')\n",
    "            land_area_str = val.replace('m²', '').replace(',', '').strip()\n",
    "            if land_area_str and land_area_str != '−':\n",
    "                land_area = int(land_area_str)\n",
    "    \n",
    "    return pd.Series([bedrooms, bathrooms, car_spaces, land_area])\n",
    "\n",
    "# Apply the function to create four new columns\n",
    "df[['bedrooms', 'bathrooms', 'car_spaces', 'land_area']] = df['property_features'].apply(parse_property_features)\n",
    "\n",
    "# Convert to nullable integer type (Int64) to preserve NaNs while using integer type\n",
    "df['bedrooms'] = df['bedrooms'].astype('Int64')\n",
    "df['bathrooms'] = df['bathrooms'].astype('Int64')\n",
    "df['car_spaces'] = df['car_spaces'].astype('Int64')\n",
    "df['land_area'] = df['land_area'].astype('Int64')\n",
    "\n",
    "# drop land_area since mostly missing and difficult to fill\n",
    "df = df.drop(columns=['land_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1c146f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before imputation:\n",
      "bedrooms       144\n",
      "bathrooms       58\n",
      "car_spaces    2037\n",
      "dtype: int64\n",
      "Property type: apartment / unit / flat, bedrooms imputed with 2\n",
      "Property type: house, bedrooms imputed with 4\n",
      "Property type: studio, bedrooms imputed with 1\n",
      "Property type: apartment / unit / flat, bathrooms imputed with 1\n",
      "Property type: house, bathrooms imputed with 2\n",
      "Property type: townhouse, car_spaces imputed with 2\n",
      "Property type: apartment / unit / flat, car_spaces imputed with 1\n",
      "Property type: house, car_spaces imputed with 2\n",
      "Property type: studio, car_spaces imputed with 1\n",
      "Property type: villa, car_spaces imputed with 1\n",
      "Property type: terrace, car_spaces imputed with 2\n",
      "Property type: new apartments / off the plan, car_spaces imputed with 1\n",
      "Property type: semi-detached, car_spaces imputed with 1\n",
      "\n",
      "After imputation:\n",
      "bedrooms      0\n",
      "bathrooms     0\n",
      "car_spaces    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# handle missing values by imputing by mode grouped by property_type\n",
    "print(\"Before imputation:\")\n",
    "print(df[['bedrooms', 'bathrooms', 'car_spaces']].isnull().sum())\n",
    "\n",
    "# Impute bedrooms using property_type mode\n",
    "df['bedrooms'] = preprocessor.impute_by_property_type_mode(df, 'bedrooms')\n",
    "\n",
    "# Impute bathrooms using property_type mode\n",
    "df['bathrooms'] = preprocessor.impute_by_property_type_mode(df, 'bathrooms')\n",
    "\n",
    "# Impute car_spaces using property_type mode\n",
    "df['car_spaces'] = preprocessor.impute_by_property_type_mode(df, 'car_spaces')\n",
    "\n",
    "print(\"\\nAfter imputation:\")\n",
    "print(df[['bedrooms', 'bathrooms', 'car_spaces']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2cf28302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xj/ny4zmnqd609bv2rpjzzr11rm0000gn/T/ipykernel_84097/3178825508.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['appointment_only'] = df['appointment_only'].fillna(df['appointment_only'].mode()[0])\n"
     ]
    }
   ],
   "source": [
    "# impute appointment_only\n",
    "df['appointment_only'] = df['appointment_only'].fillna(df['appointment_only'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "be12f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime, using mixed format to handle inconsistent datetime formats\n",
    "df[\"updated_date\"] = pd.to_datetime(df[\"updated_date\"], format=\"mixed\")\n",
    "df[\"first_listed_date\"] = pd.to_datetime(df[\"first_listed_date\"], format=\"mixed\")\n",
    "df[\"last_sold_date\"] = pd.to_datetime(df[\"last_sold_date\"], format=\"mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bf68cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert updated_date to year and quarter\n",
    "df['year'] = df['updated_date'].dt.year\n",
    "df['quarter'] = df['updated_date'].dt.quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad317f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with unknown price frequencies: 86\n"
     ]
    }
   ],
   "source": [
    "# Extract weekly rent from rental_price column\n",
    "df['rental_price'] = preprocessor.extract_rental_price(df['rental_price'])\n",
    "\n",
    "# Check how many rows have unknown frequencies (NaN in weekly_rent)\n",
    "unknown_count = df['rental_price'].isna().sum()\n",
    "print(f\"Rows with unknown price frequencies: {unknown_count}\")\n",
    "\n",
    "# Drop rows with unknown frequencies\n",
    "df = df[df['rental_price'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "18f52765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12650, 29)\n"
     ]
    }
   ],
   "source": [
    "# only keep relevant columns\n",
    "relevant_columns = [\n",
    "    'property_id','rental_price', \n",
    "    'suburb', 'postcode', 'property_type', 'year', 'quarter',\n",
    "    'bedrooms', 'bathrooms', 'car_spaces',\n",
    "    'age_0_to_19', 'age_20_to_39', 'age_40_to_59', 'age_60_plus',\n",
    "    'agency_name', 'appointment_only', 'avg_days_on_market',\n",
    "    'description', 'family_percentage',\n",
    "    'first_listed_date',\n",
    "    'latitude', 'longitude', 'listing_status', 'long_term_resident', \n",
    "    'median_rent_price', 'median_sold_price', 'number_sold',\n",
    "    'renter_percentage', 'single_percentage'\n",
    "]\n",
    "\n",
    "df = df[relevant_columns]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5deca9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12650 entries, 0 to 14143\n",
      "Data columns (total 29 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   property_id         12650 non-null  float64       \n",
      " 1   rental_price        12650 non-null  float64       \n",
      " 2   suburb              12650 non-null  object        \n",
      " 3   postcode            12650 non-null  int64         \n",
      " 4   property_type       12650 non-null  object        \n",
      " 5   year                12650 non-null  int32         \n",
      " 6   quarter             12650 non-null  int32         \n",
      " 7   bedrooms            12650 non-null  Int64         \n",
      " 8   bathrooms           12650 non-null  Int64         \n",
      " 9   car_spaces          12650 non-null  Int64         \n",
      " 10  age_0_to_19         12650 non-null  float64       \n",
      " 11  age_20_to_39        12650 non-null  float64       \n",
      " 12  age_40_to_59        12650 non-null  float64       \n",
      " 13  age_60_plus         12650 non-null  float64       \n",
      " 14  agency_name         12650 non-null  object        \n",
      " 15  appointment_only    12650 non-null  bool          \n",
      " 16  avg_days_on_market  12649 non-null  float64       \n",
      " 17  description         12650 non-null  object        \n",
      " 18  family_percentage   12650 non-null  float64       \n",
      " 19  first_listed_date   12650 non-null  datetime64[ns]\n",
      " 20  latitude            12650 non-null  float64       \n",
      " 21  longitude           12650 non-null  float64       \n",
      " 22  listing_status      12650 non-null  object        \n",
      " 23  long_term_resident  12650 non-null  float64       \n",
      " 24  median_rent_price   12649 non-null  float64       \n",
      " 25  median_sold_price   12649 non-null  float64       \n",
      " 26  number_sold         12650 non-null  float64       \n",
      " 27  renter_percentage   12650 non-null  float64       \n",
      " 28  single_percentage   12650 non-null  float64       \n",
      "dtypes: Int64(3), bool(1), datetime64[ns](1), float64(16), int32(2), int64(1), object(5)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c4af856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12649 entries, 0 to 14143\n",
      "Data columns (total 29 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   property_id         12649 non-null  float64       \n",
      " 1   rental_price        12649 non-null  float64       \n",
      " 2   suburb              12649 non-null  object        \n",
      " 3   postcode            12649 non-null  int64         \n",
      " 4   property_type       12649 non-null  object        \n",
      " 5   year                12649 non-null  int32         \n",
      " 6   quarter             12649 non-null  int32         \n",
      " 7   bedrooms            12649 non-null  Int64         \n",
      " 8   bathrooms           12649 non-null  Int64         \n",
      " 9   car_spaces          12649 non-null  Int64         \n",
      " 10  age_0_to_19         12649 non-null  float64       \n",
      " 11  age_20_to_39        12649 non-null  float64       \n",
      " 12  age_40_to_59        12649 non-null  float64       \n",
      " 13  age_60_plus         12649 non-null  float64       \n",
      " 14  agency_name         12649 non-null  object        \n",
      " 15  appointment_only    12649 non-null  bool          \n",
      " 16  avg_days_on_market  12649 non-null  float64       \n",
      " 17  description         12649 non-null  object        \n",
      " 18  family_percentage   12649 non-null  float64       \n",
      " 19  first_listed_date   12649 non-null  datetime64[ns]\n",
      " 20  latitude            12649 non-null  float64       \n",
      " 21  longitude           12649 non-null  float64       \n",
      " 22  listing_status      12649 non-null  object        \n",
      " 23  long_term_resident  12649 non-null  float64       \n",
      " 24  median_rent_price   12649 non-null  float64       \n",
      " 25  median_sold_price   12649 non-null  float64       \n",
      " 26  number_sold         12649 non-null  float64       \n",
      " 27  renter_percentage   12649 non-null  float64       \n",
      " 28  single_percentage   12649 non-null  float64       \n",
      "dtypes: Int64(3), bool(1), datetime64[ns](1), float64(16), int32(2), int64(1), object(5)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12649, 29)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove remaining rows where any of the columns is null\n",
    "df = df.dropna()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "140eb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to processed/domain/live_listings.csv\n",
    "df.to_csv(\"../data/processed/domain/live_listings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "01c3f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['property_id', 'longitude', 'latitude', 'suburb']]\n",
    "\n",
    "# convert longitude and latitude to Point\n",
    "df['coordinates'] = df.apply(lambda row: Point(row['latitude'], row['longitude']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into batches and save to output directory\n",
    "\n",
    "batch_size = 500\n",
    "for output_dir in [\"../data/raw/missing_routes\", \"../data/raw/missing_poi\", \"../data/raw/missing_isochrones/driving\", \"../data/raw/missing_isochrones/walking\"]:\n",
    "    batch_files = preprocessor.split_into_batches(df[['property_id', 'coordinates']], batch_size, output_dir)\n",
    "    print(f\"\\nCreated {len(batch_files)} batch files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984f6e1",
   "metadata": {},
   "source": [
    "Now we have set up the input files we can call the `./run_` scripts associated with making the API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4b36f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data/processed/isochrones/driving/isochrones_driving_combined.csv \n",
    "isochrones_driving = pd.read_csv(\"../data/processed/isochrones/driving/isochrones_driving_combined.csv\")\n",
    "\n",
    "# read in data/processed/isochrones/walking/isochrones_walking_combined.csv\n",
    "isochrones_walking = pd.read_csv(\"../data/processed/isochrones/walking/isochrones_walking_combined.csv\")\n",
    "\n",
    "# drop coordinates column\n",
    "isochrones_driving = isochrones_driving.drop(columns=['coordinates'])\n",
    "isochrones_walking = isochrones_walking.drop(columns=['coordinates'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a9280d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>suburb</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>driving_5min</th>\n",
       "      <th>driving_10min</th>\n",
       "      <th>driving_15min</th>\n",
       "      <th>walking_5min</th>\n",
       "      <th>walking_10min</th>\n",
       "      <th>walking_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17732837.0</td>\n",
       "      <td>144.996157</td>\n",
       "      <td>-37.796893</td>\n",
       "      <td>collingwood-abbotsford</td>\n",
       "      <td>POINT (-37.796893 144.9961565)</td>\n",
       "      <td>POLYGON ((144.979355 -37.798607, 144.981794 -3...</td>\n",
       "      <td>POLYGON ((144.959626 -37.793, 144.962138 -37.8...</td>\n",
       "      <td>POLYGON ((144.936435 -37.785708, 144.936258 -3...</td>\n",
       "      <td>POLYGON ((144.992561 -37.795622, 144.992555 -3...</td>\n",
       "      <td>POLYGON ((144.987942 -37.794449, 144.988173 -3...</td>\n",
       "      <td>POLYGON ((144.983394 -37.794141, 144.983946 -3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17744154.0</td>\n",
       "      <td>145.007683</td>\n",
       "      <td>-37.811065</td>\n",
       "      <td>collingwood-abbotsford</td>\n",
       "      <td>POINT (-37.8110653 145.0076834)</td>\n",
       "      <td>POLYGON ((144.980748 -37.80875, 144.981582 -37...</td>\n",
       "      <td>POLYGON ((144.958496 -37.803735, 144.95771 -37...</td>\n",
       "      <td>POLYGON ((144.925411 -37.825948, 144.926093 -3...</td>\n",
       "      <td>POLYGON ((145.002942 -37.810891, 145.003013 -3...</td>\n",
       "      <td>POLYGON ((144.998207 -37.810222, 144.998236 -3...</td>\n",
       "      <td>POLYGON ((144.993536 -37.809922, 144.993569 -3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17750349.0</td>\n",
       "      <td>145.001906</td>\n",
       "      <td>-37.802110</td>\n",
       "      <td>collingwood-abbotsford</td>\n",
       "      <td>POINT (-37.80210950000001 145.0019064)</td>\n",
       "      <td>POLYGON ((144.981316 -37.798493, 144.982206 -3...</td>\n",
       "      <td>POLYGON ((144.962479 -37.799088, 144.966284 -3...</td>\n",
       "      <td>POLYGON ((144.938168 -37.789279, 144.939602 -3...</td>\n",
       "      <td>POLYGON ((144.997121 -37.801603, 144.997121 -3...</td>\n",
       "      <td>POLYGON ((144.992658 -37.801436, 144.993016 -3...</td>\n",
       "      <td>POLYGON ((144.989197 -37.800257, 144.989158 -3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17739910.0</td>\n",
       "      <td>144.999856</td>\n",
       "      <td>-37.809205</td>\n",
       "      <td>collingwood-abbotsford</td>\n",
       "      <td>POINT (-37.8092053 144.999856)</td>\n",
       "      <td>POLYGON ((144.979348 -37.808679, 144.97928 -37...</td>\n",
       "      <td>POLYGON ((144.956625 -37.806511, 144.957063 -3...</td>\n",
       "      <td>POLYGON ((144.939835 -37.78925, 144.940454 -37...</td>\n",
       "      <td>POLYGON ((144.996383 -37.809856, 144.996346 -3...</td>\n",
       "      <td>POLYGON ((144.991645 -37.809719, 144.991735 -3...</td>\n",
       "      <td>POLYGON ((144.987048 -37.808997, 144.987011 -3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17751219.0</td>\n",
       "      <td>144.993940</td>\n",
       "      <td>-37.808042</td>\n",
       "      <td>collingwood-abbotsford</td>\n",
       "      <td>POINT (-37.8080424 144.9939399)</td>\n",
       "      <td>POLYGON ((144.97256 -37.807922, 144.973631 -37...</td>\n",
       "      <td>POLYGON ((144.951863 -37.805174, 144.95199 -37...</td>\n",
       "      <td>POLYGON ((144.936452 -37.785721, 144.936286 -3...</td>\n",
       "      <td>POLYGON ((144.990064 -37.807245, 144.991239 -3...</td>\n",
       "      <td>POLYGON ((144.985368 -37.806814, 144.986504 -3...</td>\n",
       "      <td>POLYGON ((144.980519 -37.806187, 144.9818 -37....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   property_id   longitude   latitude                  suburb  \\\n",
       "0   17732837.0  144.996157 -37.796893  collingwood-abbotsford   \n",
       "1   17744154.0  145.007683 -37.811065  collingwood-abbotsford   \n",
       "2   17750349.0  145.001906 -37.802110  collingwood-abbotsford   \n",
       "3   17739910.0  144.999856 -37.809205  collingwood-abbotsford   \n",
       "4   17751219.0  144.993940 -37.808042  collingwood-abbotsford   \n",
       "\n",
       "                              coordinates  \\\n",
       "0          POINT (-37.796893 144.9961565)   \n",
       "1         POINT (-37.8110653 145.0076834)   \n",
       "2  POINT (-37.80210950000001 145.0019064)   \n",
       "3          POINT (-37.8092053 144.999856)   \n",
       "4         POINT (-37.8080424 144.9939399)   \n",
       "\n",
       "                                        driving_5min  \\\n",
       "0  POLYGON ((144.979355 -37.798607, 144.981794 -3...   \n",
       "1  POLYGON ((144.980748 -37.80875, 144.981582 -37...   \n",
       "2  POLYGON ((144.981316 -37.798493, 144.982206 -3...   \n",
       "3  POLYGON ((144.979348 -37.808679, 144.97928 -37...   \n",
       "4  POLYGON ((144.97256 -37.807922, 144.973631 -37...   \n",
       "\n",
       "                                       driving_10min  \\\n",
       "0  POLYGON ((144.959626 -37.793, 144.962138 -37.8...   \n",
       "1  POLYGON ((144.958496 -37.803735, 144.95771 -37...   \n",
       "2  POLYGON ((144.962479 -37.799088, 144.966284 -3...   \n",
       "3  POLYGON ((144.956625 -37.806511, 144.957063 -3...   \n",
       "4  POLYGON ((144.951863 -37.805174, 144.95199 -37...   \n",
       "\n",
       "                                       driving_15min  \\\n",
       "0  POLYGON ((144.936435 -37.785708, 144.936258 -3...   \n",
       "1  POLYGON ((144.925411 -37.825948, 144.926093 -3...   \n",
       "2  POLYGON ((144.938168 -37.789279, 144.939602 -3...   \n",
       "3  POLYGON ((144.939835 -37.78925, 144.940454 -37...   \n",
       "4  POLYGON ((144.936452 -37.785721, 144.936286 -3...   \n",
       "\n",
       "                                        walking_5min  \\\n",
       "0  POLYGON ((144.992561 -37.795622, 144.992555 -3...   \n",
       "1  POLYGON ((145.002942 -37.810891, 145.003013 -3...   \n",
       "2  POLYGON ((144.997121 -37.801603, 144.997121 -3...   \n",
       "3  POLYGON ((144.996383 -37.809856, 144.996346 -3...   \n",
       "4  POLYGON ((144.990064 -37.807245, 144.991239 -3...   \n",
       "\n",
       "                                       walking_10min  \\\n",
       "0  POLYGON ((144.987942 -37.794449, 144.988173 -3...   \n",
       "1  POLYGON ((144.998207 -37.810222, 144.998236 -3...   \n",
       "2  POLYGON ((144.992658 -37.801436, 144.993016 -3...   \n",
       "3  POLYGON ((144.991645 -37.809719, 144.991735 -3...   \n",
       "4  POLYGON ((144.985368 -37.806814, 144.986504 -3...   \n",
       "\n",
       "                                       walking_15min  \n",
       "0  POLYGON ((144.983394 -37.794141, 144.983946 -3...  \n",
       "1  POLYGON ((144.993536 -37.809922, 144.993569 -3...  \n",
       "2  POLYGON ((144.989197 -37.800257, 144.989158 -3...  \n",
       "3  POLYGON ((144.987048 -37.808997, 144.987011 -3...  \n",
       "4  POLYGON ((144.980519 -37.806187, 144.9818 -37....  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge isochrones_driving with df on property_id\n",
    "merged_df = pd.merge(df, isochrones_driving, on='property_id', how='left')\n",
    "\n",
    "# rename the columns 5min, 10min 15min to driving_5min, driving_10min, driving_15min\n",
    "merged_df = merged_df.rename(columns={'5min': 'driving_5min', '10min': 'driving_10min', '15min': 'driving_15min'})\n",
    "\n",
    "# merge isochrones_walking with df on property_id\n",
    "merged_df = pd.merge(merged_df, isochrones_walking, on='property_id', how='left')\n",
    "\n",
    "# rename the columns 5min, 10min 15min to walking_5min, walking_10min, walking_15min\n",
    "merged_df = merged_df.rename(columns={'5min': 'walking_5min', '10min': 'walking_10min', '15min': 'walking_15min'})\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "61fb3cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15005 entries, 0 to 15004\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   property_id           15005 non-null  float64\n",
      " 1   longitude             15005 non-null  float64\n",
      " 2   latitude              15005 non-null  float64\n",
      " 3   suburb                15005 non-null  object \n",
      " 4   coordinates           15005 non-null  object \n",
      " 5   driving_5min          13863 non-null  object \n",
      " 6   driving_10min         13863 non-null  object \n",
      " 7   driving_15min         13863 non-null  object \n",
      " 8   walking_5min          9695 non-null   object \n",
      " 9   walking_10min         9695 non-null   object \n",
      " 10  walking_15min         9695 non-null   object \n",
      " 11  driving_5min_imputed  15005 non-null  object \n",
      "dtypes: float64(3), object(9)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows where the driving_5min is null\n",
    "null_driving_5min = merged_df[merged_df['driving_5min'].isnull()]\n",
    "\n",
    "# get the rows where the walking_5min is null\n",
    "null_walking_5min = merged_df[merged_df['walking_5min'].isnull()]\n",
    "\n",
    "# use preprocessor to split into batches\n",
    "preprocessor.split_into_batches(null_driving_5min[['property_id', 'coordinates']], 500, \"../data/raw/missing_isochrones/driving\")\n",
    "preprocessor.split_into_batches(null_walking_5min[['property_id', 'coordinates']], 500, \"../data/raw/missing_isochrones/walking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the driving_5min, driving_10min, driving_15min, walking_5min, walking_10min, walking_15min columns\n",
    "merged_df = merged_df.drop(columns=['driving_5min', 'driving_10min', 'driving_15min', 'walking_5min', 'walking_10min', 'walking_15min'])\n",
    "\n",
    "# rename the columns driving_5min_imputed, driving_10min_imputed, driving_15min_imputed, walking_5min_imputed, walking_10min_imputed, walking_15min_imputed to driving_5min, driving_10min, driving_15min, walking_5min, walking_10min, walking_15min\n",
    "merged_df = merged_df.rename(columns={'driving_5min_imputed': 'driving_5min', 'driving_10min_imputed': 'driving_10min', 'driving_15min_imputed': 'driving_15min', 'walking_5min_imputed': 'walking_5min', 'walking_10min_imputed': 'walking_10min', 'walking_15min_imputed': 'walking_15min'})\n",
    "\n",
    "# save the merged dataframe with property_id and driving_5min, driving_10min, driving_15min to data/curated/rent_features/cleaned_isochrones_driving.csv\n",
    "merged_df[['property_id', 'driving_5min', 'driving_10min', 'driving_15min']].to_csv(\"../data/curated/rent_features/cleaned_isochrones_driving.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated dataframe with imputed columns\n",
    "merged_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
